{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, detectron2\n",
    "\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import  get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader, build_detection_train_loader, DatasetMapper\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data.detection_utils import read_image\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data.datasets import register_coco_instances, load_coco_json\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renombrado completado.\n"
     ]
    }
   ],
   "source": [
    "#Directorio donde se encuentran las imágenes originales\n",
    "directorio = 'frames/'\n",
    "frames = (sorted(os.listdir(directorio), key=lambda x: int(x.split(\".\")[0])))\n",
    "# Itera sobre todas las imágenes en el directorio\n",
    "for i, nombre_archivo in enumerate(frames):\n",
    "#    # Construye el nuevo nombre de archivo con el formato deseado\n",
    "    nuevo_nombre = f\"{i+1:05d}.jpg\"\n",
    "    # Renombra el archivo\n",
    "    #os.rename(os.path.join(directorio, nombre_archivo), os.path.join(directorio, nuevo_nombre))\n",
    "\n",
    "print(\"Renombrado completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector:\n",
    "    \n",
    "    def __init__(self, model_type = \"OD\", outpath:str=\"./random\"):\n",
    "        self.cfg = get_cfg()\n",
    "        \n",
    "        #Load model config and pretrained model\n",
    "        if model_type == \"OD\":\n",
    "            self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "            self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")      \n",
    "            \n",
    "            \n",
    "        elif model_type == \"IS\":\n",
    "            \n",
    "            self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "            self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")      \n",
    "            \n",
    "        elif model_type == \"KP\":\n",
    "            \n",
    "            self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "            self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")      \n",
    "            \n",
    "                \n",
    "        self.cfg.OUTPUT_DIR =   os.path.join(outpath, \"23_no_shuffled\")   \n",
    "        #self.cfg.defrost()\n",
    "\n",
    "        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "        self.cfg.MODEL.DEVICE = \"cuda\"\n",
    "        self.cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "        self.cfg.SOLVER.CHECKPOINT_PERIOD = 250\n",
    "        self.cfg.SOLVER.MAX_ITER = 500\n",
    "        #self.cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n",
    "\n",
    "        #self.cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "        #self.cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "\n",
    "        self.__loader_mapping = loader_mapping = {\n",
    "                                                'first': data_loaders.load_sequential_hold_out,\n",
    "                                                'random': data_loaders.load_random_data,\n",
    "                                                '13': data_loaders.load_13,\n",
    "                                                '12': data_loaders.load_12,\n",
    "                                                '23': data_loaders.load_23\n",
    "                                            }\n",
    "        \n",
    "        self._datasets = {\"train\": None, \"val\": None}\n",
    "        self._metadata = { \"train\": None, \"val\": None }\n",
    "\n",
    "\n",
    "    def prepare_dataset(self, kind:str = \"first\", kfolds:bool=False, select_folds:str=\"12\"):\n",
    "        self.loader = self.__loader_mapping.get(kind if not kfolds else select_folds, None)\n",
    "\n",
    "\n",
    "        if self.loader is None:\n",
    "            raise ValueError(\"Invalid split or folds value\")\n",
    "\n",
    "        for data_type in ['train', 'val']:\n",
    "            static = set([\"1\",\"2\",\"3\"])\n",
    "            query  = set(list(select_folds))\n",
    "            test_name = list(static  - query)[-1]\n",
    "            \n",
    "            if data_type == \"val\":\n",
    "                dataset_name = os.path.join(\"datafolds\", f\"{data_type}_{kind if not kfolds else test_name}\")\n",
    "                self.cfg.DATASETS.TEST = (f\"{data_type}_dataset\",)\n",
    "           \n",
    "            else:\n",
    "                dataset_name = os.path.join(\"datafolds\", f\"{data_type}_{kind if not kfolds else select_folds}\")\n",
    "                self.cfg.DATASETS.TRAIN = (f\"{data_type}_dataset\",)\n",
    "            \n",
    "            #register_coco_instances(f\"{data_type}_dataset\", {}, dataset_name+\".json\", \"./frames\")\n",
    "            print(data_type)\n",
    "            print(dataset_name+\".json\")\n",
    "            self._metadata[data_type] = MetadataCatalog.get(f\"{data_type}_dataset\").set(thing_classes=[\"car\", \"bike\"])            \n",
    "            self._datasets[data_type] = DatasetCatalog.register(f\"{data_type}_dataset\", lambda d=data_type: self.loader(data_type) )\n",
    "\n",
    "        os.makedirs(self.cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    def fit_(self):\n",
    "\n",
    "        trainer = DefaultTrainer(self.cfg)\n",
    "        #DefaultTrainer.build_train_loader = lambda: dataloader\n",
    "\n",
    "        trainer.resume_or_load(resume=False)\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "\n",
    "    def evaluate_(self):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train model with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "datafolds/train_23.json\n",
      "val\n",
      "datafolds/val_1.json\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(model_type=\"OD\", outpath=\"./23_no_shuffle_keep\")\n",
    "detector.prepare_dataset(kind = \"23\", kfolds=True, select_folds=\"23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/06 02:03:17 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[03/06 02:03:17 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 707 images left.\n",
      "\u001b[32m[03/06 02:03:18 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|\n",
      "|    car     | 7154         |    bike    | 245          |\n",
      "|            |              |            |              |\n",
      "|   total    | 7399         |            |              |\u001b[0m\n",
      "\u001b[32m[03/06 02:03:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[03/06 02:03:18 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[03/06 02:03:18 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/06 02:03:18 d2.data.common]: \u001b[0mSerializing 707 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/06 02:03:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[32m[03/06 02:03:18 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/06 02:03:18 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[03/06 02:03:18 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl ...\n",
      "\u001b[32m[03/06 02:03:18 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cboned/miniconda3/envs/detectron/lib/python3.9/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/06 02:03:38 d2.utils.events]: \u001b[0m eta: 0:08:04  iter: 19  total_loss: 0.9277  loss_cls: 0.2981  loss_box_reg: 0.5612  loss_rpn_cls: 0.01959  loss_rpn_loc: 0.04867    time: 0.9790  last_time: 1.0147  data_time: 0.0229  last_data_time: 0.0095   lr: 0.00077924  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:03:58 d2.utils.events]: \u001b[0m eta: 0:07:45  iter: 39  total_loss: 0.5615  loss_cls: 0.1541  loss_box_reg: 0.365  loss_rpn_cls: 0.01159  loss_rpn_loc: 0.02979    time: 0.9846  last_time: 0.9067  data_time: 0.0088  last_data_time: 0.0089   lr: 0.0015784  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:04:18 d2.utils.events]: \u001b[0m eta: 0:07:26  iter: 59  total_loss: 0.4514  loss_cls: 0.1221  loss_box_reg: 0.2867  loss_rpn_cls: 0.00422  loss_rpn_loc: 0.02448    time: 0.9877  last_time: 0.9057  data_time: 0.0095  last_data_time: 0.0077   lr: 0.0023776  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:04:38 d2.utils.events]: \u001b[0m eta: 0:07:07  iter: 79  total_loss: 0.4052  loss_cls: 0.1011  loss_box_reg: 0.2567  loss_rpn_cls: 0.005243  loss_rpn_loc: 0.02041    time: 0.9919  last_time: 1.0246  data_time: 0.0097  last_data_time: 0.0082   lr: 0.0031768  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:04:58 d2.utils.events]: \u001b[0m eta: 0:06:47  iter: 99  total_loss: 0.3604  loss_cls: 0.08476  loss_box_reg: 0.2388  loss_rpn_cls: 0.01054  loss_rpn_loc: 0.01838    time: 0.9916  last_time: 1.0304  data_time: 0.0094  last_data_time: 0.0098   lr: 0.003976  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:05:18 d2.utils.events]: \u001b[0m eta: 0:06:27  iter: 119  total_loss: 0.3209  loss_cls: 0.08824  loss_box_reg: 0.2157  loss_rpn_cls: 0.002664  loss_rpn_loc: 0.01622    time: 0.9968  last_time: 1.0235  data_time: 0.0094  last_data_time: 0.0081   lr: 0.0047752  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:05:39 d2.utils.events]: \u001b[0m eta: 0:06:07  iter: 139  total_loss: 0.3046  loss_cls: 0.08358  loss_box_reg: 0.2033  loss_rpn_cls: 0.001526  loss_rpn_loc: 0.01499    time: 0.9990  last_time: 1.0210  data_time: 0.0091  last_data_time: 0.0083   lr: 0.0055744  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:05:59 d2.utils.events]: \u001b[0m eta: 0:05:46  iter: 159  total_loss: 0.2805  loss_cls: 0.07461  loss_box_reg: 0.1768  loss_rpn_cls: 0.00184  loss_rpn_loc: 0.01474    time: 0.9979  last_time: 1.0189  data_time: 0.0094  last_data_time: 0.0069   lr: 0.0063736  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:06:18 d2.utils.events]: \u001b[0m eta: 0:05:26  iter: 179  total_loss: 0.2676  loss_cls: 0.07242  loss_box_reg: 0.1783  loss_rpn_cls: 0.001795  loss_rpn_loc: 0.01622    time: 0.9978  last_time: 1.0276  data_time: 0.0094  last_data_time: 0.0099   lr: 0.0071728  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:06:38 d2.utils.events]: \u001b[0m eta: 0:05:06  iter: 199  total_loss: 0.2654  loss_cls: 0.07617  loss_box_reg: 0.1757  loss_rpn_cls: 0.001486  loss_rpn_loc: 0.01403    time: 0.9964  last_time: 1.0254  data_time: 0.0098  last_data_time: 0.0101   lr: 0.007972  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:06:58 d2.utils.events]: \u001b[0m eta: 0:04:45  iter: 219  total_loss: 0.2433  loss_cls: 0.06905  loss_box_reg: 0.1623  loss_rpn_cls: 0.00146  loss_rpn_loc: 0.01329    time: 0.9968  last_time: 1.0196  data_time: 0.0096  last_data_time: 0.0083   lr: 0.0087712  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:07:19 d2.utils.events]: \u001b[0m eta: 0:04:25  iter: 239  total_loss: 0.2474  loss_cls: 0.06706  loss_box_reg: 0.1582  loss_rpn_cls: 0.001973  loss_rpn_loc: 0.01412    time: 0.9987  last_time: 1.0237  data_time: 0.0095  last_data_time: 0.0066   lr: 0.0095704  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:07:39 d2.utils.events]: \u001b[0m eta: 0:04:05  iter: 259  total_loss: 0.2477  loss_cls: 0.06286  loss_box_reg: 0.1658  loss_rpn_cls: 0.001082  loss_rpn_loc: 0.01413    time: 0.9998  last_time: 1.0256  data_time: 0.0087  last_data_time: 0.0077   lr: 0.01037  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:07:59 d2.utils.events]: \u001b[0m eta: 0:03:44  iter: 279  total_loss: 0.2478  loss_cls: 0.05955  loss_box_reg: 0.1608  loss_rpn_cls: 0.003589  loss_rpn_loc: 0.01394    time: 0.9995  last_time: 1.0258  data_time: 0.0090  last_data_time: 0.0086   lr: 0.011169  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:08:19 d2.utils.events]: \u001b[0m eta: 0:03:24  iter: 299  total_loss: 0.2438  loss_cls: 0.06787  loss_box_reg: 0.1551  loss_rpn_cls: 0.001592  loss_rpn_loc: 0.01521    time: 0.9996  last_time: 0.9141  data_time: 0.0092  last_data_time: 0.0094   lr: 0.011968  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:08:39 d2.utils.events]: \u001b[0m eta: 0:03:04  iter: 319  total_loss: 0.2465  loss_cls: 0.07374  loss_box_reg: 0.1622  loss_rpn_cls: 0.001646  loss_rpn_loc: 0.01373    time: 0.9998  last_time: 1.0206  data_time: 0.0093  last_data_time: 0.0071   lr: 0.012767  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:08:59 d2.utils.events]: \u001b[0m eta: 0:02:43  iter: 339  total_loss: 0.2916  loss_cls: 0.07392  loss_box_reg: 0.1898  loss_rpn_cls: 0.002295  loss_rpn_loc: 0.01758    time: 0.9997  last_time: 1.0253  data_time: 0.0088  last_data_time: 0.0092   lr: 0.013566  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:09:19 d2.utils.events]: \u001b[0m eta: 0:02:23  iter: 359  total_loss: 0.2454  loss_cls: 0.0624  loss_box_reg: 0.1593  loss_rpn_cls: 0.006333  loss_rpn_loc: 0.01472    time: 0.9987  last_time: 1.0279  data_time: 0.0090  last_data_time: 0.0096   lr: 0.014366  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:09:39 d2.utils.events]: \u001b[0m eta: 0:02:02  iter: 379  total_loss: 0.25  loss_cls: 0.06839  loss_box_reg: 0.1659  loss_rpn_cls: 0.001497  loss_rpn_loc: 0.01539    time: 0.9993  last_time: 1.0246  data_time: 0.0088  last_data_time: 0.0093   lr: 0.015165  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:09:59 d2.utils.events]: \u001b[0m eta: 0:01:42  iter: 399  total_loss: 0.2678  loss_cls: 0.07115  loss_box_reg: 0.1775  loss_rpn_cls: 0.001829  loss_rpn_loc: 0.0157    time: 0.9994  last_time: 1.0247  data_time: 0.0099  last_data_time: 0.0094   lr: 0.015964  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:10:19 d2.utils.events]: \u001b[0m eta: 0:01:21  iter: 419  total_loss: 0.2437  loss_cls: 0.05814  loss_box_reg: 0.1616  loss_rpn_cls: 0.002421  loss_rpn_loc: 0.01391    time: 0.9990  last_time: 1.0264  data_time: 0.0087  last_data_time: 0.0079   lr: 0.016763  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:10:39 d2.utils.events]: \u001b[0m eta: 0:01:01  iter: 439  total_loss: 0.232  loss_cls: 0.06209  loss_box_reg: 0.1528  loss_rpn_cls: 0.004631  loss_rpn_loc: 0.01495    time: 0.9996  last_time: 1.0337  data_time: 0.0092  last_data_time: 0.0143   lr: 0.017562  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:11:00 d2.utils.events]: \u001b[0m eta: 0:00:40  iter: 459  total_loss: 0.2021  loss_cls: 0.05207  loss_box_reg: 0.1318  loss_rpn_cls: 0.002563  loss_rpn_loc: 0.01198    time: 1.0008  last_time: 1.0245  data_time: 0.0090  last_data_time: 0.0091   lr: 0.018362  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:11:20 d2.utils.events]: \u001b[0m eta: 0:00:20  iter: 479  total_loss: 0.2267  loss_cls: 0.05663  loss_box_reg: 0.1496  loss_rpn_cls: 0.002423  loss_rpn_loc: 0.01381    time: 1.0002  last_time: 1.0250  data_time: 0.0087  last_data_time: 0.0096   lr: 0.019161  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:11:41 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 499  total_loss: 0.2222  loss_cls: 0.06032  loss_box_reg: 0.1432  loss_rpn_cls: 0.001131  loss_rpn_loc: 0.01325    time: 1.0001  last_time: 1.0217  data_time: 0.0097  last_data_time: 0.0115   lr: 0.01996  max_mem: 6212M\n",
      "\u001b[32m[03/06 02:11:41 d2.engine.hooks]: \u001b[0mOverall training speed: 498 iterations in 0:08:18 (1.0001 s / it)\n",
      "\u001b[32m[03/06 02:11:41 d2.engine.hooks]: \u001b[0mTotal training time: 0:08:19 (0:00:01 on hooks)\n",
      "\u001b[32m[03/06 02:11:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[03/06 02:11:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/06 02:11:41 d2.data.common]: \u001b[0mSerializing 707 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/06 02:11:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/06 02:11:41 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
     ]
    }
   ],
   "source": [
    "detector.fit_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "#Change config on your custom path\n",
    "config_yaml_path = \"configs/config_32_no_shuffle.yaml\"\n",
    "\n",
    "\n",
    "\n",
    "with open(config_yaml_path, 'w') as file:\n",
    "    yaml.dump(detector.cfg, file)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataloaders to create the videos and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetCatalog.get(\"train_dataset\")\n",
    "train_metadata = MetadataCatalog.get(\"train_dataset\")\n",
    "\n",
    "\n",
    "val_loader = DatasetCatalog.get(\"val_dataset\")\n",
    "val_metadata = MetadataCatalog.get(\"val_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/06 02:15:28 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./23_no_shuffle_keep/23_no_shuffled/model_final.pth ...\n"
     ]
    }
   ],
   "source": [
    "detector.cfg.MODEL.WEIGHTS = os.path.join(detector.cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "detector.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(detector.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/06 02:15:31 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'val_dataset' to COCO format ...\n",
      "\u001b[32m[03/06 02:15:31 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'val_dataset' to COCO format ...)\n",
      "\u001b[32m[03/06 02:15:31 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[03/06 02:15:32 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 707, #annotations: 7399\n",
      "\u001b[32m[03/06 02:15:32 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at './output/val_dataset_coco_format.json' ...\n",
      "\u001b[32m[03/06 02:15:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[03/06 02:15:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/06 02:15:32 d2.data.common]: \u001b[0mSerializing 707 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/06 02:15:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n"
     ]
    }
   ],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"val_dataset\", output_dir=\"./output\")\n",
    "val_loader = build_detection_test_loader(detector.cfg, \"val_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/06 02:15:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 707 batches\n",
      "\u001b[32m[03/06 02:15:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/707. Dataloading: 0.0008 s/iter. Inference: 0.1000 s/iter. Eval: 0.0003 s/iter. Total: 0.1011 s/iter. ETA=0:01:10\n",
      "\u001b[32m[03/06 02:15:41 d2.evaluation.evaluator]: \u001b[0mInference done 60/707. Dataloading: 0.0014 s/iter. Inference: 0.1004 s/iter. Eval: 0.0003 s/iter. Total: 0.1021 s/iter. ETA=0:01:06\n",
      "\u001b[32m[03/06 02:15:46 d2.evaluation.evaluator]: \u001b[0mInference done 109/707. Dataloading: 0.0014 s/iter. Inference: 0.1006 s/iter. Eval: 0.0003 s/iter. Total: 0.1023 s/iter. ETA=0:01:01\n",
      "\u001b[32m[03/06 02:15:51 d2.evaluation.evaluator]: \u001b[0mInference done 158/707. Dataloading: 0.0013 s/iter. Inference: 0.1006 s/iter. Eval: 0.0003 s/iter. Total: 0.1022 s/iter. ETA=0:00:56\n",
      "\u001b[32m[03/06 02:15:56 d2.evaluation.evaluator]: \u001b[0mInference done 207/707. Dataloading: 0.0014 s/iter. Inference: 0.1006 s/iter. Eval: 0.0003 s/iter. Total: 0.1023 s/iter. ETA=0:00:51\n",
      "\u001b[32m[03/06 02:16:01 d2.evaluation.evaluator]: \u001b[0mInference done 256/707. Dataloading: 0.0014 s/iter. Inference: 0.1007 s/iter. Eval: 0.0003 s/iter. Total: 0.1024 s/iter. ETA=0:00:46\n",
      "\u001b[32m[03/06 02:16:06 d2.evaluation.evaluator]: \u001b[0mInference done 305/707. Dataloading: 0.0014 s/iter. Inference: 0.1008 s/iter. Eval: 0.0003 s/iter. Total: 0.1025 s/iter. ETA=0:00:41\n",
      "\u001b[32m[03/06 02:16:11 d2.evaluation.evaluator]: \u001b[0mInference done 354/707. Dataloading: 0.0014 s/iter. Inference: 0.1007 s/iter. Eval: 0.0003 s/iter. Total: 0.1025 s/iter. ETA=0:00:36\n",
      "\u001b[32m[03/06 02:16:16 d2.evaluation.evaluator]: \u001b[0mInference done 403/707. Dataloading: 0.0014 s/iter. Inference: 0.1008 s/iter. Eval: 0.0003 s/iter. Total: 0.1025 s/iter. ETA=0:00:31\n",
      "\u001b[32m[03/06 02:16:21 d2.evaluation.evaluator]: \u001b[0mInference done 452/707. Dataloading: 0.0014 s/iter. Inference: 0.1008 s/iter. Eval: 0.0003 s/iter. Total: 0.1025 s/iter. ETA=0:00:26\n",
      "\u001b[32m[03/06 02:16:26 d2.evaluation.evaluator]: \u001b[0mInference done 501/707. Dataloading: 0.0014 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1026 s/iter. ETA=0:00:21\n",
      "\u001b[32m[03/06 02:16:31 d2.evaluation.evaluator]: \u001b[0mInference done 550/707. Dataloading: 0.0014 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1026 s/iter. ETA=0:00:16\n",
      "\u001b[32m[03/06 02:16:36 d2.evaluation.evaluator]: \u001b[0mInference done 599/707. Dataloading: 0.0014 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1026 s/iter. ETA=0:00:11\n",
      "\u001b[32m[03/06 02:16:41 d2.evaluation.evaluator]: \u001b[0mInference done 648/707. Dataloading: 0.0014 s/iter. Inference: 0.1010 s/iter. Eval: 0.0003 s/iter. Total: 0.1027 s/iter. ETA=0:00:06\n",
      "\u001b[32m[03/06 02:16:46 d2.evaluation.evaluator]: \u001b[0mInference done 697/707. Dataloading: 0.0014 s/iter. Inference: 0.1010 s/iter. Eval: 0.0003 s/iter. Total: 0.1027 s/iter. ETA=0:00:01\n",
      "\u001b[32m[03/06 02:16:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:12.156780 (0.102787 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/06 02:16:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:10 (0.101031 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/06 02:16:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[03/06 02:16:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[03/06 02:16:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[03/06 02:16:47 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[03/06 02:16:48 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.29 seconds.\n",
      "\u001b[32m[03/06 02:16:48 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[03/06 02:16:48 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.820\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.985\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.969\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.850\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.828\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.853\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.853\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[03/06 02:16:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 82.003 | 98.514 | 96.855 | 84.950 |  nan  |  nan  |\n",
      "\u001b[32m[03/06 02:16:48 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[03/06 02:16:48 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|\n",
      "| car        | 88.183 | bike       | 75.822 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bbox',\n",
       "              {'AP': 82.00272154450771,\n",
       "               'AP50': 98.51442692763926,\n",
       "               'AP75': 96.85492612932295,\n",
       "               'APs': 84.95049504950495,\n",
       "               'APm': nan,\n",
       "               'APl': nan,\n",
       "               'AP-car': 88.18298474456753,\n",
       "               'AP-bike': 75.82245834444791})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inference_on_dataset(predictor.model, val_loader , evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Folder with the inference of all the validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "detector.cfg.MODEL.WEIGHTS = os.path.join(detector.cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "detector.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(detector.cfg)\n",
    "\n",
    "plot_path = os.path.join(\"plot_results\", \"first_no_shuffle\")\n",
    "\n",
    "os.makedirs(plot_path, exist_ok=True)\n",
    "\n",
    "for d in (val_loader): \n",
    "    for i in d:#select number of images for display\n",
    "        im = cv2.imread(i[\"file_name\"])\n",
    "        outputs = predictor(im)\n",
    "        \n",
    "        v = Visualizer(im[:, :, ::-1],\n",
    "                    metadata=val_metadata,\n",
    "                    scale=0.5,\n",
    "                    instance_mode=ColorMode.IMAGE    # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "        )\n",
    "        instances = outputs[\"instances\"]\n",
    "        out = v.draw_instance_predictions(instances.to(\"cpu\"))\n",
    "        boxes = v._convert_boxes(outputs[\"instances\"].pred_boxes.to('cpu')).squeeze()\n",
    "\n",
    "        for box in boxes:\n",
    "            out = v.draw_text(f\"Car\", (box[0], box[1]))\n",
    "            \n",
    "        plt.imshow(out.get_image()[:, :, ::-1])\n",
    "        cv2.imwrite(plot_path+f\"/{i['file_name'].split('/')[-1]}\", out.get_image()[:, :, ::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
